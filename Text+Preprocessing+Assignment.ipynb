{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    sent = nltk.tokenize.sent_tokenize(input_string)\n",
    "    mining = []\n",
    "    for i in sent:\n",
    "        wordTokens = nltk.tokenize.word_tokenize(i)\n",
    "        lower_tokens = [token.lower() for token in wordTokens]\n",
    "        stop = stopwords.words('english')\n",
    "        tokens = [token for token in lower_tokens if token not in stop]\n",
    "        lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "        mining.append(tokens)\n",
    "    print(mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''The quick brown fox. Jumped over the lazy dog.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "process_text(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Sravan\\\\Downloads\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sravan\\\\Downloads\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab (1)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sherlock.txt\", \"r\") as f:\n",
    "    string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', '.'], ['perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', '.'], ['tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.'], ['least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.'], ['companion', 'sat', 'front', 'trap', ',', 'arm', 'folded', ',', 'hat', 'pulled', 'eye', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.'], ['suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadow', '.']]\n"
     ]
    }
   ],
   "source": [
    "process_text(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sravan\\\\Downloads\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab (1)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Sravan\\\\Downloads\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sherlock.txt\", \"r\") as f:\n",
    "    string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordTokens = nltk.tokenize.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "print(list(wordTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No of Words in a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No of Sentences in a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\", 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.', 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.', 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.', 'My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.', 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = nltk.tokenize.sent_tokenize(string)\n",
    "print(sent)\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No fo Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(wordTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', '.', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'companion', 'sat', 'front', 'trap', ',', 'arms', 'folded', ',', 'hat', 'pulled', 'eyes', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.tokenize.sent_tokenize(string)\n",
    "wordTokens = nltk.tokenize.word_tokenize(string)\n",
    "lower_tokens = [token.lower() for token in wordTokens]\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram and Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most common 10 unigrams are: [(('the',), 13), ((',',), 11), (('and',), 7), (('a',), 6), (('.',), 6), (('his',), 5), (('in',), 4), (('of',), 4), (('we',), 3), (('were',), 3)]\n",
      "the most common 10 bigrams are: [(('of', 'the'), 4), (('in', 'the'), 3), ((',', 'and'), 3), (('we', 'were'), 2), (('was', 'a'), 2), ((',', 'his'), 2), (('At', 'Waterloo'), 1), (('Waterloo', 'we'), 1), (('were', 'fortunate'), 1), (('fortunate', 'in'), 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram_freq = nltk.FreqDist()\n",
    "unigram_freq = nltk.FreqDist()\n",
    "corpus_tokens = nltk.tokenize.word_tokenize(string)\n",
    "unigrams = list(nltk.ngrams(corpus_tokens, 1))\n",
    "bigrams = list(nltk.ngrams(corpus_tokens,2))\n",
    "for unigram_token in unigrams:\n",
    "    unigram_freq[unigram_token] +=1\n",
    "print('the most common 10 unigrams are:',unigram_freq.most_common(10))\n",
    "for bigram_token in bigrams:\n",
    "    bigram_freq[bigram_token] += 1\n",
    "print('the most common 10 bigrams are:',bigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGE  NUMBER OF CHARACTERS PER WORD IN A TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.902439024390244"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "for i in wordTokens:\n",
    "    a =  len(i)\n",
    "    b.append(a)\n",
    "b\n",
    "\n",
    "c=sum(b)/len(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
