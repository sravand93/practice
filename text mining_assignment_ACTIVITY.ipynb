{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LENOVO'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\\\INSOFE\\\\text mining\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\INSOFE\\\\text mining\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20171028_Batch31_CSE7306_Day01_Lab_Preprocessing.ipynb',\n",
       " '20171028_Batch31_CSE7306_Day01_Lab_TFIDF.ipynb',\n",
       " 'mytfidf.csv',\n",
       " 'notes.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'sherlock.txt',\n",
       " 'wordcloud-1.2.1-cp27-cp27m-win_amd64.whl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignemnt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    sentence = nltk.tokenize.sent_tokenize(input_string)\n",
    "    out = []\n",
    "    for sent in sentence:\n",
    "        wordTokens = nltk.tokenize.word_tokenize(sent)\n",
    "        lower_tokens = [token.lower() for token in wordTokens]\n",
    "        stop = stopwords.words('english')\n",
    "        tokens = [token for token in lower_tokens if token not in stop]\n",
    "        lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "        out.append(tokens)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''\n",
    "The quick brown fox. Jumped over the lazy dog.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "process_text(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('notes.txt','r') as f:\n",
    "    string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead,', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes.', 'It', 'was', 'a', 'perfect', 'day,', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots,', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap,', 'his', 'arms', 'folded,', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes,', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast,', 'buried', 'in', 'the', 'deepest', 'thought.', 'Suddenly,', 'however,', 'he', 'started,', 'tapped', 'me', 'on', 'the', 'shoulder,', 'and', 'pointed', 'over', 'the', 'meadows.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = string.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To find the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 13, 'and': 7, 'a': 6, 'his': 5, 'in': 4, 'of': 4, 'we': 3, 'were': 3, 'was': 3, 'for': 2, 'at': 2, 'me': 2, 'upon': 2, 'over': 2, 'At': 1, 'Waterloo': 1, 'fortunate': 1, 'catching': 1, \"don't\": 1, 'train': 1, 'Leatherhead,': 1, 'where': 1, 'hired': 1, 'trap': 1, 'station': 1, 'inn': 1, 'drove': 1, 'four': 1, 'or': 1, 'five': 1, 'miles': 1, 'through': 1, 'lovely': 1, 'Surrey': 1, 'lanes.': 1, 'It': 1, 'perfect': 1, 'day,': 1, 'with': 1, 'bright': 1, 'sun': 1, 'few': 1, 'fleecy': 1, 'clouds': 1, 'heavens.': 1, 'The': 1, 'trees': 1, 'wayside': 1, 'hedges': 1, 'just': 1, 'throwing': 1, 'out': 1, 'their': 1, 'first': 1, 'green': 1, 'shoots,': 1, 'air': 1, 'full': 1, 'pleasant': 1, 'smell': 1, 'moist': 1, 'earth.': 1, 'To': 1, 'least': 1, 'there': 1, 'strange': 1, 'contrast': 1, 'between': 1, 'sweet': 1, 'promise': 1, 'spring': 1, 'this': 1, 'sinister': 1, 'quest': 1, 'which': 1, 'engaged.': 1, 'My': 1, 'companion': 1, 'sat': 1, 'front': 1, 'trap,': 1, 'arms': 1, 'folded,': 1, 'hat': 1, 'pulled': 1, 'down': 1, 'eyes,': 1, 'chin': 1, 'sunk': 1, 'breast,': 1, 'buried': 1, 'deepest': 1, 'thought.': 1, 'Suddenly,': 1, 'however,': 1, 'he': 1, 'started,': 1, 'tapped': 1, 'on': 1, 'shoulder,': 1, 'pointed': 1, 'meadows.': 1})\n"
     ]
    }
   ],
   "source": [
    "wordcount = Counter(words)\n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "freq=0\n",
    "for word in words:\n",
    "    freq=freq+1\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "no.of sentences in the text are: 6\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "sentences = nltk.tokenize.sent_tokenize(string)\n",
    "for sent in sentences:\n",
    "    num=num+1\n",
    "    print('\\n')\n",
    "    print(sent)\n",
    "    \n",
    "print(\"no.of sentences in the text are:\",num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 102\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "unique_words = set(words)\n",
    "for word in unique_words:\n",
    "    count = count+1\n",
    "print (\"unique words =\",count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', '.', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'companion', 'sat', 'front', 'trap', ',', 'arms', 'folded', ',', 'hat', 'pulled', 'eyes', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "# print(stop)\n",
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most common unigrams and bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most commeon 10 unigrams are: [(('the',), 13), ((',',), 11), (('and',), 7), (('a',), 6), (('.',), 6), (('his',), 5), (('in',), 4), (('of',), 4), (('we',), 3), (('were',), 3)]\n",
      "the most commeon 10 bigrams are: [(('of', 'the'), 4), (('in', 'the'), 3), ((',', 'and'), 3), (('we', 'were'), 2), (('was', 'a'), 2), ((',', 'his'), 2), (('At', 'Waterloo'), 1), (('Waterloo', 'we'), 1), (('were', 'fortunate'), 1), (('fortunate', 'in'), 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram_freq = nltk.FreqDist()\n",
    "unigram_freq = nltk.FreqDist()\n",
    "corpus_tokens = nltk.tokenize.word_tokenize(string)\n",
    "unigrams = list(nltk.ngrams(corpus_tokens, 1))\n",
    "bigrams = list(nltk.ngrams(corpus_tokens,2))\n",
    "for unigram_token in unigrams:\n",
    "    unigram_freq[unigram_token] +=1\n",
    "print('the most commeon 10 unigrams are:',unigram_freq.most_common(10))\n",
    "for bigram_token in bigrams:\n",
    "    bigram_freq[bigram_token] += 1\n",
    "print('the most commeon 10 bigrams are:',bigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
