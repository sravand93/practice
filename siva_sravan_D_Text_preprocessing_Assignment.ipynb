{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(input_string):\n",
    "    sentence = nltk.tokenize.sent_tokenize(input_string)\n",
    "    out = []\n",
    "    for sent in sentence:\n",
    "        wordTokens = nltk.tokenize.word_tokenize(sent)\n",
    "        lower_tokens = [token.lower() for token in wordTokens]\n",
    "        stop = stopwords.words('english')\n",
    "        tokens = [token for token in lower_tokens if token not in stop]\n",
    "        lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "        out.append(tokens)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''\n",
    "The quick brown fox. Jumped over the lazy dog.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "process_text(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\\\INSOFE\\\\text mining\\\\20171028_Batch31_CSE7306c_Day01_TextMining_Lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('notes.txt','r') as f:\n",
    "    string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No. of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "sentences = nltk.tokenize.sent_tokenize(string)\n",
    "for sent in sentences:\n",
    "    num=num+1\n",
    "    print('\\n')\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no.of sentences in the text are: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"no.of sentences in the text are:\",num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences using len(): 6\n"
     ]
    }
   ],
   "source": [
    "len(sentences)\n",
    "print('# of sentences using len():',len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find the no.of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 104\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "unique_words = set(tokens)\n",
    "for word in unique_words:\n",
    "    count = count+1\n",
    "print (\"unique words =\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find the average character per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.816091954022989"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "for i in tokens:\n",
    "    a =  len(i)\n",
    "    b.append(a)\n",
    "b\n",
    "\n",
    "c=sum(b)/len(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect', 'day', ',', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', '.', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', ',', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', '.', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', '.', 'companion', 'sat', 'front', 'trap', ',', 'arms', 'folded', ',', 'hat', 'pulled', 'eyes', ',', 'chin', 'sunk', 'upon', 'breast', ',', 'buried', 'deepest', 'thought', '.', 'suddenly', ',', 'however', ',', 'started', ',', 'tapped', 'shoulder', ',', 'pointed', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 most common unigrams and bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most commeon 10 unigrams are: [(('the',), 13), ((',',), 11), (('and',), 7), (('a',), 6), (('.',), 6), (('his',), 5), (('in',), 4), (('of',), 4), (('we',), 3), (('were',), 3)]\n",
      "the most commeon 10 bigrams are: [(('of', 'the'), 4), (('in', 'the'), 3), ((',', 'and'), 3), (('we', 'were'), 2), (('was', 'a'), 2), ((',', 'his'), 2), (('At', 'Waterloo'), 1), (('Waterloo', 'we'), 1), (('were', 'fortunate'), 1), (('fortunate', 'in'), 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram_freq = nltk.FreqDist()\n",
    "unigram_freq = nltk.FreqDist()\n",
    "corpus_tokens = nltk.tokenize.word_tokenize(string)\n",
    "unigrams = list(nltk.ngrams(corpus_tokens, 1))\n",
    "bigrams = list(nltk.ngrams(corpus_tokens,2))\n",
    "for unigram_token in unigrams:\n",
    "    unigram_freq[unigram_token] +=1\n",
    "print('the most commeon 10 unigrams are:',unigram_freq.most_common(10))\n",
    "for bigram_token in bigrams:\n",
    "    bigram_freq[bigram_token] += 1\n",
    "print('the most commeon 10 bigrams are:',bigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
