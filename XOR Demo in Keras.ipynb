{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "## What is it?\n",
    "Some of the famous frameworks to perform deep-learning experiments are theano, tensorflow and torch. These are quite inaccessible to the common crowd in terms of coding complexity. \n",
    "Each framework requires one to build the entire network inclusive of how and where each weight is calculated in the neural-network. Although this gives a lot of control over the neural net, debugging becomes an issue and prototyping a fast solution for an experimenter is very difficult.\n",
    "\n",
    "\n",
    "Keras simplifies the issue by using theano/tensorflow in the backend while exposing an API with set of common functions, which is user friendly.\n",
    "\n",
    "\n",
    "## Let's understand how keras works\n",
    "We shall build a simple one-hidden layer neural network with the following inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      " [0 1 1 0]\n",
      "(4, 2) (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#CREATE XOR DATA\n",
    "X = np.asarray([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.asarray([0, 1, 1, 0])\n",
    "print(X,'\\n', y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we import a set of dependencies.\n",
    "* Most of our neural nets are a sequence of layers, so we import an object which stores these layers and automatically knows how to forward and back propagate.\n",
    "* A dense object is a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#LOAD REQUIRED MODULES\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A net is constructed in the following way\n",
    "Can you draw a graph and see how it looks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim = 2, activation = 'sigmoid'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "## In case the model has more than two categories (which is the case in MNIST Activity) one needs to use \n",
    "## model.compile(loss='categorical_crossentropy', ...)\n",
    "## categorical_crossentropy is an extension of binary crossentropy for more categories\n",
    "%time model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28de9179048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]]\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      " [[ 0.00618563]\n",
      " [ 0.99274921]\n",
      " [ 0.99490428]\n",
      " [ 0.00604742]]\n",
      "4/4 [==============================] - 0s\n",
      "[0.0061641419306397438, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_data = [[0,1], [1,1]]\n",
    "predictions = model.predict_classes(test_data, verbose = False) #GIVES CLASSES\n",
    "print(predictions)\n",
    "\n",
    "prediction_probabilities = model.predict(X) #GIVES PROBABILITIES\n",
    "print(X,'\\n'*2,prediction_probabilities)\n",
    "\n",
    "print(model.evaluate(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "There are many more things that can go under the hood.\n",
    "* **kernel_initializer** is responsible for creating random weights using a specified distribution - ```RandomUniform```\n",
    "* activation is different for the layers\n",
    "* we have given our own optimizer - ```opt``` with our own learning rate\n",
    "* we have used a **learning rate reducer** ```lrs``` which reduces learning rate from ```x``` to ```9*x/10``` if loss does not decrease by ```epsilon=0.001``` for 10 epochs straight.\n",
    "* we have used an **early stopper** which stops training before 100 epochs if the loss doesn't decrease by 0.01 in 10 epochs.\n",
    "* we have **plotted** the loss and accuracy of the model at every epoch using the ```history``` object\n",
    "\n",
    "\n",
    "* using right combinations of hyperparameters we have reduce the number of epochs from 5000 to 100 and only 10 hidden neurons. Try other combinations to make the model more simple (for example, try to fit the model using less than 10 hiddens in the first layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.1)\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "lrr = ReduceLROnPlateau(monitor='loss', min_lr=1e-3, factor=0.9, epsilon=0.001, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=1e-2, patience=10, verbose=1)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = 2, kernel_initializer='RandomUniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='RandomUniform', activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, epochs=100, verbose=0, callbacks=[lrr, early_stop])\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['acc'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot more initializers:\n",
    "* Zeros\n",
    "* Ones\n",
    "* Constant\n",
    "* RandomNormal\n",
    "* RandomUniform\n",
    "* TruncatedNormal\n",
    "* VarianceScaling\n",
    "* Orthogonal\n",
    "* Identity\n",
    "* lecun_uniform\n",
    "* glorot_normal\n",
    "* glorot_uniform\n",
    "* he_normal\n",
    "* he_uniform\n",
    "\n",
    "There are a lot more loss functions:\n",
    "* mean_squared_error\n",
    "* mean_absolute_error\n",
    "* mean_absolute_percentage_error\n",
    "* mean_squared_logarithmic_error\n",
    "* squared_hinge\n",
    "* hinge\n",
    "* logcosh\n",
    "* categorical_crossentropy\n",
    "* sparse_categorical_crossentropy\n",
    "* binary_crossentropy\n",
    "* kullback_leibler_divergence\n",
    "* poisson\n",
    "* cosine_proximity\n",
    "\n",
    "There are a lot more optimizers:\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* Adam\n",
    "* Adamax\n",
    "* Nadam\n",
    "* TFOptimizer\n",
    "\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/ is a great place to understand about the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
