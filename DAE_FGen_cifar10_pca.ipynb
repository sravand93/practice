{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "tmp = np.load('E:\\\\INSOFE\\\\AI and Deep Learning\\\\DAE_FGen\\\\cifar_pca_train.npz')\n",
    "train_data = tmp['data']\n",
    "train_labels = tmp['labels']\n",
    "\n",
    "tmp = np.load('E:\\\\INSOFE\\\\AI and Deep Learning\\\\DAE_FGen\\\\cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training an autoencoder model on cifar-10 PCA reduced data\n",
    "\n",
    "input_img = Input(shape=(781,))\n",
    "crrpt_img = Dropout(0.5)(input_img)\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img)\n",
    "decoded = Dense(781, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "\n",
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 96s - loss: 0.5831 - mean_squared_error: 0.5831 - val_loss: 0.2351 - val_mean_squared_error: 0.2351\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.5380 - mean_squared_error: 0.5380 - val_loss: 0.2127 - val_mean_squared_error: 0.2127\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.5296 - mean_squared_error: 0.5296 - val_loss: 0.2049 - val_mean_squared_error: 0.2049\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 90s - loss: 0.5285 - mean_squared_error: 0.5285 - val_loss: 0.2029 - val_mean_squared_error: 0.2029\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 88s - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.2023 - val_mean_squared_error: 0.2023\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 92s - loss: 0.5253 - mean_squared_error: 0.5253 - val_loss: 0.1984 - val_mean_squared_error: 0.1984\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 87s - loss: 0.5221 - mean_squared_error: 0.5221 - val_loss: 0.1961 - val_mean_squared_error: 0.1961\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.5233 - mean_squared_error: 0.5233 - val_loss: 0.1975 - val_mean_squared_error: 0.1975\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 89s - loss: 0.5199 - mean_squared_error: 0.5199 - val_loss: 0.1971 - val_mean_squared_error: 0.1971\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 87s - loss: 0.5189 - mean_squared_error: 0.5189 - val_loss: 0.1933 - val_mean_squared_error: 0.1933\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.1947 - val_mean_squared_error: 0.1947\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 87s - loss: 0.5139 - mean_squared_error: 0.5139 - val_loss: 0.1956 - val_mean_squared_error: 0.1956\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 90s - loss: 0.5148 - mean_squared_error: 0.5148 - val_loss: 0.1972 - val_mean_squared_error: 0.1972\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.5136 - mean_squared_error: 0.5136 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 85s - loss: 0.5134 - mean_squared_error: 0.5134 - val_loss: 0.1978 - val_mean_squared_error: 0.1978\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 0.2015 - val_mean_squared_error: 0.2015\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.5090 - mean_squared_error: 0.5090 - val_loss: 0.2021 - val_mean_squared_error: 0.2021\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.5086 - mean_squared_error: 0.5086 - val_loss: 0.2032 - val_mean_squared_error: 0.2032\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.5078 - mean_squared_error: 0.5078 - val_loss: 0.2051 - val_mean_squared_error: 0.2051\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.5069 - mean_squared_error: 0.5069 - val_loss: 0.2091 - val_mean_squared_error: 0.2091\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.5067 - mean_squared_error: 0.5067 - val_loss: 0.2086 - val_mean_squared_error: 0.2086\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.5055 - mean_squared_error: 0.5055 - val_loss: 0.2106 - val_mean_squared_error: 0.2106\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.5039 - mean_squared_error: 0.5039 - val_loss: 0.2123 - val_mean_squared_error: 0.2123\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.5029 - mean_squared_error: 0.5029 - val_loss: 0.2147 - val_mean_squared_error: 0.2147\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 85s - loss: 0.5043 - mean_squared_error: 0.5043 - val_loss: 0.2176 - val_mean_squared_error: 0.2176\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.5016 - mean_squared_error: 0.5016 - val_loss: 0.2199 - val_mean_squared_error: 0.2199\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.5010 - mean_squared_error: 0.5010 - val_loss: 0.2247 - val_mean_squared_error: 0.2247\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.5002 - mean_squared_error: 0.5002 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.4982 - mean_squared_error: 0.4982 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.4995 - mean_squared_error: 0.4995 - val_loss: 0.2294 - val_mean_squared_error: 0.2294\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.4969 - mean_squared_error: 0.4969 - val_loss: 0.2311 - val_mean_squared_error: 0.2311\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 86s - loss: 0.4958 - mean_squared_error: 0.4958 - val_loss: 0.2348 - val_mean_squared_error: 0.2348\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 87s - loss: 0.4959 - mean_squared_error: 0.4959 - val_loss: 0.2361 - val_mean_squared_error: 0.2361\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 88s - loss: 0.4957 - mean_squared_error: 0.4957 - val_loss: 0.2379 - val_mean_squared_error: 0.2379\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 88s - loss: 0.4954 - mean_squared_error: 0.4954 - val_loss: 0.2427 - val_mean_squared_error: 0.2427\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 84s - loss: 0.4925 - mean_squared_error: 0.4925 - val_loss: 0.2411 - val_mean_squared_error: 0.2411\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4928 - mean_squared_error: 0.4928 - val_loss: 0.2431 - val_mean_squared_error: 0.2431\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4928 - mean_squared_error: 0.4928 - val_loss: 0.2464 - val_mean_squared_error: 0.2464\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4924 - mean_squared_error: 0.4924 - val_loss: 0.2463 - val_mean_squared_error: 0.2463\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4914 - mean_squared_error: 0.4914 - val_loss: 0.2483 - val_mean_squared_error: 0.2483\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4917 - mean_squared_error: 0.4917 - val_loss: 0.2502 - val_mean_squared_error: 0.2502\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4900 - mean_squared_error: 0.4900 - val_loss: 0.2525 - val_mean_squared_error: 0.2525\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4902 - mean_squared_error: 0.4902 - val_loss: 0.2558 - val_mean_squared_error: 0.2558\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 83s - loss: 0.4897 - mean_squared_error: 0.4897 - val_loss: 0.2534 - val_mean_squared_error: 0.2534\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4909 - mean_squared_error: 0.4909 - val_loss: 0.2562 - val_mean_squared_error: 0.2562\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4903 - mean_squared_error: 0.4903 - val_loss: 0.2581 - val_mean_squared_error: 0.2581\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4894 - mean_squared_error: 0.4894 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4886 - mean_squared_error: 0.4886 - val_loss: 0.2585 - val_mean_squared_error: 0.2585\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4882 - mean_squared_error: 0.4882 - val_loss: 0.2564 - val_mean_squared_error: 0.2564\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 82s - loss: 0.4878 - mean_squared_error: 0.4878 - val_loss: 0.2626 - val_mean_squared_error: 0.2626\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train_data, train_data,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data),\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "autoencoder = load_model(\"E:\\\\INSOFE\\\\AI and Deep Learning\\\\DAE_FGen\\\\DAE_l1_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 781)\n",
      "(50000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(htrain_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ..., 9 1 1]\n",
      "[3 8 8 ..., 5 1 7]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting labels into one-hot vectors for MLP training\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = keras.utils.to_categorical(test_labels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training a two hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = mlp.fit(train_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training an MLP on autoencoder features\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "history = mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(htest_data, test_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
